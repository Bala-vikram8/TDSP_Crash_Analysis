# -*- coding: utf-8 -*-
"""Transportation Data Science Project (TDSP).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sc3kEEqu0ZS1kHII7LjKrmaqaHgZtZcU

# ðŸš— **Welcome to the <font color='crimson'>** **Explorer Transportation Data Science Project! ðŸš—**</font>
 Hosted by the [Northeast Big Data Innovation Hub](https://nebigdatahub.org/about) & [National Student Data Corps](https://nebigdatahub.org/nsdc), in collaboration with the [U.S. Department of Transportation Federal Highway Administration](https://highways.dot.gov/).


---

## <font color='crimson'>**Project Information and Background:**</font>

**Project Description:**

By participating in this project, you are joining a
community of transportation data science learners interested in making roads safer for vulnerable road users.

The Explorer TDSP has six Milestones, including guided transportation research into a community of interest. Each Milestone can take 1-5 hours, or less, depending on your level of experience.

To learn more about this project, including key highlights, incentives, and important links, [review the TDSP Webpage here](https://nebigdatahub.org/nsdc/tdsp/)!

## <font color='crimson'>**How to Get Started:**</font>

In order to work within this Google Colab Notebook, **please start by clicking on "File" in the top left corner of your notebook, and then "Save a copy in Drive." Rename the file to "Explorer TDSP - Your Full Name."** This will save a copy of the notebook in your personal Google Drive.

You may now begin!

---
---

## <font color='crimson'>**A Quick Introduction to Google Colab**</font>

Read below for a Google Colab QuickStart:
- Google Colab is a Python Notebook environment built by Google that's free for all.
- Colab Notebooks are made up of cells; cells can be either *text* or *code* cells. You can click the +code or +text button at the top of the Notebook to create a new cell.
- Text cells use a format called [Markdown](https://www.markdownguide.org/getting-started/). Knowledge of Markdown is not required for this project. However, if you'd like to learn more, [check out this Cheatsheet!](https://www.markdownguide.org/cheat-sheet/)
- Python code is executed in *code* cells. When you want to run your code, hover your cursor over the square brackets in the top left corner of your code cell. You'll notice a play button pop up! (â–¶) Click the play button to run the code in that cell. Code cells run one at a time.
- The memory shared across your notebook is called the *Runtime*. You can think of a Runtime as a "code session" where everything you create and execute is temporarily stored.
- Runtimes will persist for a short period of time, so you are safe if you need to refresh the page, but Google will shutdown a Runtime after enough time has passed. Everything that you have printed out will remain within your Notebook even if the runtime is disconnected.

If this is your first time using Google Colab, we highly recommend reviewing the [NSDC's *Using Google Colab Guide*](https://nebigdatahub.org/wp-content/uploads/2023/04/NSDC-Data-Science-Projects-Introduction-Using-Google-Colab.pdf) before continuing. For a more comprehensive guide, see [Colab's *Welcome To Colaboratory* walkthrough.](https://colab.research.google.com/github/prites18/NoteNote/blob/master/Welcome_To_Colaboratory.ipynb)

## <font color='crimson'>**An Introduction to Python Programming**</font>

Python is a programming language often used to analyze data.

Python is open-source, which means it's free to use and distribute, even for commercial purposes. Python's versatility allows it to be used for web development, data visualization, artificial intelligence, scientific computing, and more.

Python's extensive standard library, along with its powerful third-party packages, enable developers and data scientists to perform a vast array of tasks.

For those looking to dive deeper into Python, here are some valuable resources:
- [The Official Python Documentation](https://docs.python.org/3/) â€“ Offers comprehensive guides and reference materials for Python leaners.
- [Real Python](https://realpython.com/) â€“ Provides tutorials and articles for Python developers of all skill levels.
- [PyCon](https://pycon.org/) â€“ The largest annual gathering for the Python community, which is useful for learning from experts and discovering the latest developments in the Python ecosystem.
- [Python for Everybody](https://www.py4e.com/) â€“ A book and website by Dr. Charles Severance that offers a free course on Python for beginners.

**Let's review some essential Python Functions!**

Here are some key functions you'll frequently encounter:

1. **`head()`**: This function is crucial for getting a quick overview of your dataset. By default, it returns the first five rows, offering a snapshot of your data's structure and values.

2. **`describe()`**: This provides a summary of the statistical characteristics of your dataset. It's particularly useful for gaining insights into the distribution, mean, standard deviation, and range of numerical columns.

3. **`sum()`**: This calculates the total sum of a column or a series of numbers, proving essential for quick calculations and aggregations in data analysis.

4. **`isnull()`**: This helps identify missing or null values in your dataset, allowing for effective data cleaning and preprocessing.

5. **`value_counts()`**: Understanding the frequency of various values in your dataset is a common task in data science. The `value_counts()` function makes this easy by counting the occurrence of each unique value in a column.

Now that you've reviewed these important concepts, let's dive in to the project!

## <font color='crimson'>**Milestone #1 - Data Preparation**</font>
GOAL: The main goal of this milestone is to set up your environment, install the required packages, learn how to access data and do some basic exploratory data analysis.

**Step 1:** Setting up libraries and installing packages

A **library** is a collection of code that you can use in your programs, while a **package** is a folder that contains libraries or other packages, organized for easy use.

To install a library, we'll use the following format:
```python
 import <library name> as <shortname>
```
We use a *short name* since it is easier to refer to the package to access functions and also to refer to subpackages within the library. Think of it as a nickname for easier reference!
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import folium

"""These are the libraries that will help us throughout this project. We invite you to research each library for a better understanding.

We encourage you to read more about the important and most commonly used libraries like Pandas, Matplotlib, and Seaborn and write a few lines in your own words about what they do. [You may use the Data Science Resource Repository (DSRR) to find resources to get started!](https://nebigdatahub.org/nsdc/data-science-resource-repository/)

**TO DO:** Write a few lines about what each library does:




> * Pandas:
> * Matplotlib:
> * Seaborn:

**Step 2:** Letâ€™s access our data. We will be using the [NYC OpenData Motor Vehicle Collisions - Crashes dataset](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95). According to NYC Open Data, "each row represents a crash event. The Motor Vehicle Collisions data tables contain information from all police reported motor vehicle collisions in NYC." If you need a reminder on how to upload your dataset, [please review helpful hints here.](https://nebigdatahub.org/wp-content/uploads/2023/04/NSDC-Data-Science-Projects-Introduction-Using-Google-Colab.pdf)

Since this is a large dataset, we highly recommend that you upload your data by [mounting your Google Drive](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA).

To mount your Google Drive, begin by finding the folder icon on the left side of your screen. When you click on this folder icon, you will open the Files window. Click on the icon at the top of the Files window that says "Mount Drive" as you hover over it.
"""

from google.colab import drive
drive.mount('/content/drive')

"""Next, we will read the data using the `pd.read_csv` function.
Ensure that you've downloaded the dataset from NYC OpenData, and uploaded the dataset to your Google Drive.

Hint: Your data's file path describes the location of where your data lives. To locate your data's file path, click on the folder/file-shaped icon on the left side of the Notebook. You'll notice a folder labeled "drive." Search for your Motor Vehicle Collisions Dataset within that folder. Right click on your Dataset to "copy path." Paste that path below.
"""

# TODO: Read the data using pandas read_csv function
data = pd.read_csv("/content/drive/MyDrive/TSDP/Motor_Vehicle_Collisions.csv")

"""**Step 3:** Let's see what the data looks like. We can use the `head` function which returns the first 5 rows of the dataset."""

# TODO: Print the first 5 rows of the data using head function of pandas
# Display the last 5 rows of the DataFrame
print(data.tail())


# Get a concise summary of the DataFrame
data.info()


# Randomly select 5 rows from the DataFrame
print(data.sample(5))


# Get the distribution of data in the 'BOROUGH' column [optional]
print(data['BOROUGH'].value_counts())


# To view the first 5 rows of the dataset
print(data.head())


# Print all column names to check for typos or different naming
print(data.columns)


# Get the number of unique values in each column
print(data.nunique())

# TODO: Describe the data using the describe function of pandas
desc_stats = data.describe()
desc_stats

# Here we need to include all the values including missing,0 and naan values also [optional]
desc_stats = data.describe(include='all')
desc_stats

"""The information above is currently formatted in scientific notation. Need a refresher? [Review how to analyze and convert to and from scientific notation here!](https://www.mathsisfun.com/numbers/scientific-notation.html)

1. Latitude & Longitude: The latitude and longitude indicate where the crashes are occurring. However, there are some data points with latitude and longitude values of 0, which is likely due to missing or inaccurate data.

2. Number of Persons Injured: On average, each crash has around 0.305 injuries. The maximum number of injuries in a single crash is 43.

3. Number of Persons Killed: Fatalities are rare, with an average of 0.00146 deaths per crash. The maximum number of deaths in one crash is 8.

4. Number of Pedestrians, Cyclists, and Motorists Injured/Killed: These columns provide a breakdown of the injuries and fatalities by type of individual involved.

5. Collision ID: This is a unique identifier for each crash.

---
"""

# Question 1 : Analysisng the latitude and longitude
# Remove rows where latitude or longitude are 0 or missing
cleaned_data = data[(data['LATITUDE'] != 0) & (data['LONGITUDE'] != 0)]
cleaned_data = cleaned_data.dropna(subset=['LATITUDE', 'LONGITUDE'])


# Display the remaining latitude and longitude values
lat_long_analysis= cleaned_data[['LATITUDE', 'LONGITUDE']].describe()
print(lat_long_analysis)

# Question 2: Number of persons injured
# This code summarizes the statistics for the 'Number of Persons Injured' column, focusing on averages and maximum values.
injury_stats = data['NUMBER OF PERSONS INJURED'].describe()

# Print the statistics of interest
print("Average Number of Persons Injured per Crash:", injury_stats['mean'])
print("Maximum Number of Injuries in a Single Crash:", injury_stats['max'])

# Question 3: Number of persons killed
# Remove rows where 'NUMBER OF PERSONS KILLED' might be NaN
cleaned_data = data.dropna(subset=['NUMBER OF PERSONS KILLED'])


# Describe the 'NUMBER OF PERSONS KILLED' data and adjust the display format
fatalities_description = cleaned_data['NUMBER OF PERSONS KILLED'].describe()


# Print each statistic from the description
for stat, value in fatalities_description.items():
   print(f"{stat}: {value:.6f}")

# Question 4: Analyzing Breakdown of Injuries/Killed by Pedestrians, Cyclists, and Motorists
# This code gathers statistics on injuries and fatalities broken down by pedestrians, cyclists, and motorists.
pedestrian_injured = data['NUMBER OF PEDESTRIANS INJURED'].describe()
cyclist_injured = data['NUMBER OF CYCLIST INJURED'].describe()
motorist_injured = data['NUMBER OF MOTORIST INJURED'].describe()
pedestrian_killed = data['NUMBER OF PEDESTRIANS KILLED'].describe()
cyclist_killed = data['NUMBER OF CYCLIST KILLED'].describe()
motorist_killed = data['NUMBER OF MOTORIST KILLED'].describe()


# Printing the statistics
print("Pedestrian Injuries:", pedestrian_injured)
print("Cyclist Injuries:", cyclist_injured)
print("Motorist Injuries:", motorist_injured)
print("Pedestrian Fatalities:", pedestrian_killed)
print("Cyclist Fatalities:", cyclist_killed)
print("Motorist Fatalities:", motorist_killed)

# Question 5: Analyzing Collision ID
# This code checks the 'COLLISION_ID' column to ensure it's unique and provides basic information about it.
collision_id_stats = data['COLLISION_ID'].describe()


# Print each statistic from the description without displaying the series name or data type
for stat, value in collision_id_stats.items():
   print(f"{stat}: {value:.6f}")

# Print the column names of the DataFrame
print(cleaned_data.columns)

# Optional: Enhancement for Plotting
import matplotlib.pyplot as plt

# Assuming 'cleaned_data' has the appropriate latitude and longitude data
plt.figure(figsize=(10, 6))
plt.scatter(cleaned_data['LONGITUDE'], cleaned_data['LATITUDE'], alpha=0.5)

# Loop through the data points and add text annotations using 'BOROUGH' or directly from 'LOCATION'
for index, row in cleaned_data.iterrows():
    plt.text(row['LONGITUDE'], row['LATITUDE'], f"{row['BOROUGH']} ({row['LATITUDE']}, {row['LONGITUDE']})", fontsize=9, ha='right')

plt.title('Distribution of Crash Locations')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.grid(True)
plt.show()

#drive.flush_and_unmount()
print('All changes made in this colab session should now be visible in Drive.')

"""##<font color='crimson'> **Milestone #2 - Data Ethics, Pre-Processing, and Exploration** </font>
GOAL: The main goal of this milestone is to assess the dataset, find missing values, and decide what to do with those missing data points.

**Step 1:**
Before we begin assessing our data for missing values, it is important that we understand the ethical implications surrounding data processing. To best prepare yourself for this module, review one or more of the following resources:
- [Data Science Ethics Flashcard Video Series](https://youtube.com/playlist?list=PLNs9ZO9jGtUB7XTjXy-ttoo2QSLld9SrV&feature=shared)
- [What Do I Need to Understand about Data Ethics?](https://www.youtube.com/watch?v=Efy8htCDueE)
-[Introduction to Data Cleaning](https://www.youtube.com/watch?v=t8WkoGLkdTk)

**TO DO:** Based on the resources above and outside knowledge, what are some potential bias issues related to the availability of data from well-resourced communities as compared to under-resourced communities? How might bias show up in our dataset?

> Answer here: However, potential biases may arise from uneven data collection, including under-resourced areas that may have limited capacity to report incidents due to scarce resources or lack of trust in the authorities, leading to overrepresentation of well-resourced communities with better incident reporting systems. This discrepancy in data collection can result in biased predictive models that are not capable of identifying or addressing properly high risk areas and may lead to ineffectiveness or unfair distribution of traffic safety measures. Additional algorithmic biases may occur if data processing does not account for these disparities, which can skew traffic incident analyses and interventions developed from this data. These biases need to be recognized and addressed to promote equitable treatment and resource distribution across communities.

**Step 2:**
Check the dataset for missing values.
"""

import pandas as pd

#TODO: Leverage the isnull() and sum() functions to find the number of missing values in each column
missing_values = data.isnull().sum()

# TODO: Turn the missing value counts into percentages
missing_values_percentage = (missing_values / len(data)) * 100

# TODO: Return counts and percentages of missing values in each column
missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage (%)': missing_values_percentage})
missing_data.sort_by_percentage = missing_data.sort_values(by='Percentage (%)', ascending=False)

# Display the sorted missing data
print(missing_data.sort_by_percentage)

"""Here's an overview of the missing values in the dataset:

Columns like VEHICLE TYPE CODE 5, CONTRIBUTING FACTOR VEHICLE 5, VEHICLE TYPE CODE 4, and so on have a high percentage of missing values. This is expected since not all crashes involve multiple vehicles or factors.

OFF STREET NAME and CROSS STREET NAME have significant missing values. This could be due to crashes occurring in locations where these details aren't applicable or weren't recorded.

ZIP CODE, BOROUGH, and ON STREET NAME also have missing values. This might be due to incomplete data entry or crashes occurring in areas where these specifics aren't easily determinable.

LOCATION, LATITUDE, and LONGITUDE have the same count of missing values, indicating that when one is missing, the others are likely missing as well.

**Step 3:** Create a bar chart to display the top 10 contributing factors (e.g. backing up unsafely, unsafe lane changing, etc.) to crashes within the dataset.
"""

#TODO: Plot a Bar Chart


top_factors = data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().head(10)




plt.figure(figsize=(12, 7))
# TODO: Plotting the top contributing factors, fill in x as the index field of the variable 'top_factors'
sns.barplot(x=top_factors.index, y=top_factors.values, palette="magma")
plt.title('Top 10 Contributing Factors to Crashes', fontsize=16)
plt.xlabel('Contributing Factors', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""**TO DO:** Besides for "Unspecified," what are the top 3 contributing factors that cause the most crashes?

> * The most significant identifiable cause of crashes is Driver Inattention/Distraction, which means that drivers are not fully focusing on the road in many cases.
> *  This indicates a common problem of Failure to Yield Right-of-Way, when drivers do not obey yield signs or fail to yield to pedestrians and other vehicles when they are supposed to.
> *  Following Too Closely, also called tailgating, suggests that many crashes occur when drivers do not leave enough space between them and the vehicle in front of them.

**TO DO:** What recommendations would you make to new and current drivers after assessing the above data?

> 1. Increase Awareness and Focus: Stay on the job and keep your eyes on the driving task at all times. Donâ€™t drive and text or eat or do anything else that might take your eyes off the road.
2. Understand and Respect Right-of-Way Rules: Make sure you know who has right of way in different driving situations. This knowledge can help avoid accidents and misunderstandings at intersections and on highways.
3. Maintaining Safe Following Distances: Leave an adequate gap of at least three seconds between your vehicle and the one in front to allow time to react and stop or take other action. Increase this distance in bad weather, heavy traffic or when driving at higher speeds.
4. Driver Education: Periodically update your driving knowledge and skills through training and courses. Awareness programs for these top contributing factors are also helpful.
5. Use of Driver Assistance Technologies: If available, use technologies like forward collision warning, automatic emergency braking and lane departure warnings to help prevent crashes due to these factors.

**Step 4:** Now, let's create another bar chart to determine which vehicle types were involved in the most crashes.
"""

# Determine the top vehicle types involved in crashes
top_vehicle_types = data['VEHICLE TYPE CODE 1'].value_counts().head(10)
# Plotting the top vehicle types
plt.figure(figsize=(12, 7))
sns.barplot(x=top_vehicle_types.index, y=top_vehicle_types.values, palette="cividis")
plt.title('Top 10 Vehicle Types Involved in Crashes', fontsize=16)
plt.xlabel('Vehicle Type', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""**TO DO:** What are the top 3 vehicles that were most involved in crashes?

>* Sedans are the most involved in crashes, followed by Sport Utility Vehicles (SUVs), and Passenger Vehicles. These categories lead significantly in the number of incidents compared to other vehicle types.

**TO DO:** Why do you think that "Sedan[s]," "Station Wagon[s]," and "Passenger Vehicle[s]" are involved in a larger number of crashes, injuries, and deaths when compared to the rest of the vehicles? (Think outside the box!)

1) Popularity and Prevalence: These types of vehicles are among the most common on the road, especially in urban and suburban areas, leading to a higher probability of being involved in crashes simply due to their numbers.

2) Versatility and Use Case: Sedans and passenger vehicles are typically used for a wide range of activities, including commuting, school runs, and business, which increases their exposure to potential accidents.

3) Behavioral Factors: There might be a tendency for drivers of these vehicles to engage more in risky behaviors due to overconfidence in vehicle safety features or comfort with the vehicle, increasing crash risks.

**TO DO:** Review the x-axis of the bar chart you created above. </br>
1) What do you notice? </br>
2) What would you recommend we do to improve the bar chart, based on the x-axis (horizontal axis) and why? </br>
3) What recommendation would you make to those who are collecting and/or inputting data into this dataset?


> *  1)  The labels on the x-axis clearly differentiate the types of vehicles but show a mix of general categories (like Sedan) and more specific ones (like 4 dr Sedan). This can cause confusion in data interpretation and analysis.

> *  2) a) Standardize the Categories: To improve the clarity of the bar chart, standardize the vehicle type categories to either all general or all specific types. This would help in accurately comparing vehicle involvement in crashes.
b) Consolidate Similar Types: Merge similar vehicle types (e.g., â€˜Sedanâ€™ with â€˜4 dr Sedanâ€™) to avoid redundancy and provide clearer insights into data trends.

> *  3) a) Clear Guidelines for Data Entry: Provide explicit guidelines for data collectors on how to categorize each vehicle type to ensure consistency across the dataset. b) Regular Data Audits: Implement regular audits of the data to check for consistency and accuracy in how vehicle types are recorded, correcting discrepancies where necessary.

**Step 5:**  A DataFrame is a two-dimensional and potentially heterogeneous tabular data structure with labeled axes (rows and columns). DataFrames allow for storing, manipulating, and retrieving data in a structured form, serving as the primary data structure for a multitude of data processing tasks. It is used with Python libraries like pandas.

Let's graph the *types* of crashes within this dataset and their frequencies. Begin by aggregating your data, convert to [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) for simple plotting, and plot.
"""

import matplotlib.pyplot as plt
import seaborn as sns


# Aggregating data - Complete for Cyclist and Motorist
types_of_crashes = {
   'Pedestrian Injuries': data['NUMBER OF PEDESTRIANS INJURED'].sum(),
   'Cyclist Injuries': data['NUMBER OF CYCLIST INJURED'].sum(),
   'Motorist Injuries': data['NUMBER OF MOTORIST INJURED'].sum(),
   'Pedestrian Deaths': data['NUMBER OF PEDESTRIANS KILLED'].sum(),
   'Cyclist Deaths': data['NUMBER OF CYCLIST KILLED'].sum(),
   'Motorist Deaths': data['NUMBER OF MOTORIST KILLED'].sum()
}




# Converting aggregated data to DataFrame
crash_types_df = pd.DataFrame(list(types_of_crashes.items()), columns=['Crash Type', 'Count'])


# Plot
plt.figure(figsize=(12, 7))
sns.barplot(x='Count', y='Crash Type', data=crash_types_df, palette="mako")
plt.title('Types of Crashes and Their Frequencies')
plt.xlabel('Count')
plt.ylabel('Type of Crash')
plt.tight_layout()
plt.show()

"""**TO DO:** Analyze the chart above. What is a recommendation you might make to the Department of Transportation based on this data?


> *  Observations:  There is an indication that the motorist injuries are the most frequent type of crash outcome and considerably outnumber the other types of injuries and fatalities.

â€¢ The next most frequent are the pedestrian injuries, which, though less frequent than the motorist injuries, still constitute a significant portion of the crash outcomes.

â€¢ There are fewer injuries and deaths to cyclists compared to motorist and pedestrian injuries but are notable.

â€¢Deaths: Those who have been killed while being pedestrians, cyclists, and motorists are thankfully well below the injury figures, yet it is a very important area of concern.

Recommendation: More Safety Features for Drivers and Other Road Users:

â€¢ Basic Infrastructure Improvement: In light of the high ratio of motorist injuries, construction in improving road infrastructure would significantly avert these cases. This shall include but not be limited to better and clear lane markings, better lighting conditions, and more effective enforcement of traffic control: speed bumps, extended pedestrian zones.  

Safety Campaigns: Develop specialized campaigns regarding safety for all types of road users, where emphasis will still fall more on pedestrians and cyclists. Awareness campaigns will then be applied in enlightening drivers on how they need to be driving on roads carrying both pedestrians and cyclists, like not straying onto bicycle lanes, or giving priority to them while using a pedestrian crossing.

â€¢Crossings and Bike Paths: Improve the markings/stripe and provide better visibility at crosswalks; raise the grade of the crosswalk to reduce traffic collisions or install protected bike lanes.

 â€¢Increase Enforcement Data-Driven Traffic Policing: Enact enforcement on highly hazardous locations that analysis determines: enforce the speed limits; prevent driving distractions, and provide a ticket when there is parking a vehicle in either a bike lane or a crosswalk.

---

##<font color='crimson'> **Milestone #3 - Time Series Analysis**</font>
GOAL: The main goal of this milestone is to dive deeper into Time Series Analysis in order to better understand our data's trends over time.

**Step 1:**

Before we jump into Time Series Analysis (TSA), it's important to understand the basics, including Trends, Seasonality, and Residual Components.

Review one or more of the following resources and tell us what you learned about TSA!

*  [Learn about Time Series patterns here](https://otexts.com/fpp2/tspatterns.html)
* [Learn about Time Plots here](https://otexts.com/fpp2/time-plots.html)
*[Learn how to decompose Time Series Data into Trend and Seasonality](https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/)

**TO DO:** Write 3-5 sentences about TSA.
> *   Time Series Analysis (TSA) is a statistical technique to analyze the pattern of variables which are collected at different points of time to understand the characteristics of the series. It is most useful for identifying the trend, the irregularities or randomness, and the seasonal components of a time series. For instance, in TSA, trends explain the direction of the data over time; seasonality describes the systematic patterns or cycles of the series over, for instance, days, weeks, or years. In time series data, residuals are the remaining part after trend and seasonality have been accounted for; they contain information on what might be haphazard in the data. Time series decomposition is a fundamental method that breaks down a time series into these three components, enabling analysts to assess how each component influences the series. This technique is applied across economics, weather forecasting, energy consumption, and stock market analysis because understanding temporal relationships is vital for making rational decisions.

**Step 2:** Let's begin by creating a chart that displays the average number of crashes per hour of the day. This will help us understand whether additional factors are contributing to crashes - i.e. rush hour, school dismissal time, night fall, etc.
"""

import pandas as pd
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Correct the file path assuming the file is located in the folder "TSDP" inside "My Drive"
file_path_1 = "/content/drive/My Drive/TSDP/Motor_Vehicle_Collisions.csv"
data = pd.read_csv(file_path_1)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Convert 'CRASH DATE' and 'CRASH TIME' to datetime
data['CRASH DATE'] = pd.to_datetime(data['CRASH DATE'])
data['CRASH TIME'] = pd.to_datetime(data['CRASH TIME'], format='%H:%M')


# Time of Day Analysis
data['Hour of Day'] = data['CRASH TIME'].dt.hour


# Group by 'Hour of Day' and calculate the average number of crashes per hour
average_crashes_per_hour = data.groupby('Hour of Day').size() / data['Hour of Day'].nunique()


# Plot the average number of crashes
plt.figure(figsize=(12, 6))
sns.barplot(x=average_crashes_per_hour.index, y=average_crashes_per_hour.values)
plt.title('Average Number of Crashes per Hour of Day')
plt.xlabel('Hour of Day')
plt.ylabel('Average Number of Crashes')
plt.xticks(range(0, 24))
plt.show()

"""**TO DO:** Which time of the day sees the most crashes? Why do you think so?

> *  The bar chart reflects the highest flow times of crashes that occur in two time sets, between 16:00-18:00 or between 4 PM and 6 PM, because these coincide with the evening's rush hour. This would always be expected as more vehicles are usually out on the highway going home from work and can be accompanied by other complications of reduced daylight, probably driver fatigue after a workday. These conditions could give rise to reduced visibility, driver alertness, and an increase in rates of accidents. Understanding these patterns can help in the development of targeted traffic management strategies and safety campaigns aimed at crash rate reduction in these critical hours.

**Step 3:**
Plot a graph to determine how COVID-19 impacted the number of crashes per month, if at all.
"""

import matplotlib.pyplot as plt
import pandas as pd


# Convert 'CRASH DATE' to datetime format
data['CRASH DATE'] = pd.to_datetime(data['CRASH DATE'])


# Group by month and year to get the number of crashes per month
monthly_crashes = data.groupby(data['CRASH DATE'].dt.to_period("M")).size()


# Plotting the trend over time
plt.figure(figsize=(15, 7))
monthly_crashes.plot()
plt.title('Number of Crashes per Month', fontsize=16)
plt.xlabel('Date', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.tight_layout()
plt.show()

"""**TO DO:** What does your graph tell you about the impact of COVID-19 on the number of crashes per month? Why do you think this occurred?

> *    The graph above shows the impact of COVID-19 on traffic accidents, with a dramatic decrease in the number of crashes per month starting early in 2020, coinciding with the global onset of the pandemic and associated lockdowns. This sharp decline can be attributed to the stringent social distancing measures and the wide shift to remote work that reduced daily commutes and overall road traffic. While there is some gradual increase with the easing of restrictions, this has not bounced back to the pre-pandemic level. There may be some longer-term changes to commuting and therefore remote work practices. These figures do show very clearly how improvements in road safety are directly influenced by mobility-where less traffic in vehicles equates to fewer incidents on the roads.

**Step 4**: Apply time series decomposition to review trends, seasonality, and residuals. New to time series analysis? Review the [Time Series Flashcard video series](https://youtube.com/playlist?list=PLNs9ZO9jGtUAqd0CNKySksPJJaOqYPMvk&feature=shared) here to learn about trends, components, and further analysis!
"""

import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose


# Count the number of crashes per day, group by CRASH DATE
daily_crashes = data.groupby(data['CRASH DATE'].dt.date).size()


# Set plot style
sns.set(style="darkgrid")


# Plot the daily crashes time series
plt.figure(figsize=(15, 6))
plt.plot(daily_crashes.index, daily_crashes.values, label='Daily crashes')
plt.title('Daily Motor Vehicle Collisions in NYC')
plt.xlabel('Date')
plt.ylabel('Number of Crashes')
plt.legend()
plt.show()


# Decompose the time series
decomposition = seasonal_decompose(daily_crashes, model='additive', period=365)


# Plot the decomposed components
fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))
decomposition.trend.plot(ax=ax1)
ax1.set_title('Trend')
decomposition.seasonal.plot(ax=ax2)
ax2.set_title('Seasonality')
decomposition.resid.plot(ax=ax3)
ax3.set_title('Residuals')
plt.tight_layout()
plt.show()

"""The visualizations above provide valuable insights into the time series of daily motor vehicle collisions in New York City:

1. Time Series Plot: This shows the number of daily crashes over time. You might observe long-term trends, seasonal patterns, or significant outliers.

2. Decomposed Components:
  
    2.1 Trend: This graph shows the long-term trend in the data, which can indicate whether crashes are increasing, decreasing, or stable over time.

    2.2 Seasonality: This reveals any regular patterns that repeat over a specific period, such as yearly. It helps identify times of the year with higher or lower crash frequencies.

    2.3 Residuals: These are the irregular components that cannot be attributed to the trend or seasonality. They might include random or unpredictable fluctuations.

**TO DO:** Based on your *trend graph*, are we seeing an increase or a decrease in crashes between 2014 and 2022?

> *  The trend graph indicates a decrease in motor vehicle crashes starting from around 2020, following a period of relative stability or slight increase from 2014 up until the end of 2019. The notable decrease beginning in 2020 aligns with the global outbreak of COVID-19 and the associated restrictions on movement, including lockdowns and other measures that drastically reduced road traffic. This period marks a significant shift from the previous years where crash numbers were more consistent.


**TO DO:** Based on your *residual graph*, in what year(s) was there a significant unpredicted fluctuation? Why do you think so?

> *   The residual graph shows that the year 2020 had some really unpredictable fluctuations. The residual values are out of the ordinary, especially that deep negative spike in 2020. This anomaly could be attributed to the impact of the COVID-19 pandemic, which was not accounted for in typical predictive models. The lockdown measures enacted to mitigate the virus circulation were translated into an unparalleled decrease in traffic, with fewer-than-expected crashes. These deviations outline the result of external unpredictable factors like the global health crisis on road traffic.

---

##<font color='crimson'>**Milestone #4 - Geospatial Analysis**</font>
GOAL: The main goal of this milestone is to explore geospatial aspects of the dataset and get comfortable with regional analysis and geospatial visualizations.

**Step 1:** Before beginning this Milestone, we highly recommend that you review the [NSDC Geospatial Analysis Flashcard Video series](https://www.youtube.com/playlist?list=PLNs9ZO9jGtUAX_2g1-OJp8VkmVum6DqqP) if you are new to Geospatial Analysis!

Let's build a bar chart to compare and analyze the number of crashes across the five boroughs: Brooklyn (also known as Kings County), Queens, Manhattan, Bronx, and Staten Island.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


#TODO: Plot a bar chart to compare the number of crashes that occurred in each of the five boroughs.
# Assuming 'data' is your DataFrame and it has a column named 'BOROUGH'
# Ensure your data has been loaded and 'BOROUGH' is correctly spelled and formatted.
# Set style
sns.set_style("whitegrid")

# Calculate the count of crashes in each borough
borough_count = data['BOROUGH'].value_counts()

# Plotting the distribution of crashes by borough
plt.figure(figsize=(12, 7))
sns.barplot(x=borough_count.index, y=borough_count.values, palette="viridis")
plt.title('Distribution of Crashes by Borough', fontsize=16)
plt.xlabel('Borough', fontsize=14)
plt.ylabel('Number of Crashes', fontsize=14)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**TO DO:** Which borough has the highest number of crashes? Which borough has the lowest?

> * Highest: The highest number of crashes is in Brooklyn as depicted by the tallest bar in the chart.
> * Lowest: The lowest number of crashes is in Staten Island as shown by the shortest bar in the chart.


**TO DO:** Are there any reasons that you think a certain borough has a higher or lower number of crashes? What are some factors that may be causing this?

â€¢ Population Density and Traffic Volume: The number of crashes in Brooklyn is likely to be the highest because this borough has the highest population density and a large traffic volume. This implies that the more traffic, the more chances of accidents.

â€¢ Urban Planning and Road Infrastructure: Manhattan has a high density of vehicles and pedestrians, but it may not have the most crashes because of better road design and more public transportation, which reduces the use of cars.

â€¢ Geographical Size and Accessibility: The rates in Queens and the Bronx fall in the middle, which can be attributed to the blend of the urban and suburban areas within these boroughs, which influence traffic and crash frequencies.

â€¢ Economic and Social Factors: The fact that Staten Island has the fewest crashes ever recorded may be due to this being a more residential and less densely populated borough, which has less traffic congestion and, therefore, fewer opportunities for crashes than other, more urban boroughs.

**Step 2:** Heatmaps are graphical representations that use color coding to represent different values and variables. Let's leverage a heatmap to determine the most dangerous intersections in the dataset. (**Note: the below cell may take a few minutes to run**)
"""

#TODO: Create a heatmap leveraging the latitude and longitude variables to determine where the most crashes are occurring
import folium
from folium.plugins import HeatMap

# Assuming data is your DataFrame and has been properly loaded with the necessary columns.
data_geo = data.dropna(subset=['LATITUDE', 'LONGITUDE'])

# Create a base map centered around NYC
m = folium.Map(location=[40.730610, -73.935242], zoom_start=10)

# Create a heatmap
heat_data = [[row['LATITUDE'], row['LONGITUDE']] for index, row in data_geo.iterrows()]
HeatMap(heat_data, radius=8, max_zoom=13).add_to(m)

# Calculate crash counts per location along with additional details
location_crash_counts = data_geo.groupby(['LATITUDE', 'LONGITUDE', 'BOROUGH', 'ZIP CODE']).size().reset_index(name='counts')
top_locations = location_crash_counts.nlargest(10, 'counts')  # You can adjust the number as needed

# Add circle markers for top crash locations with popup details
for idx, row in top_locations.iterrows():
    popup_text = f"Borough: {row['BOROUGH']}<br>Zip Code: {row['ZIP CODE']}<br>Crashes: {row['counts']}"
    folium.CircleMarker(
        location=[row['LATITUDE'], row['LONGITUDE']],
        radius=row['counts'] / 10,  # Adjust the size of the circle by scaling the count
        color='red',
        fill=True,
        fill_color='red',
        fill_opacity=0.6,
        popup=popup_text  # Popup shows detailed location name and crash count
    ).add_to(m)

# Save the map
m.save('Heatmap.html')

"""**TO DO:** On the left side of your screen, you will see an icon that represents a folder or a file. Click on that icon to find the file titled "Heatmap.html". Click on the three dots next to your file and download your heatmap! Open the file once downloaded to see your creation.

When looking at your heatmap, where do you see a concentration of crashes?


> *  From the heatmap you have sent, there are the most crashes in the Downtown Brooklyn area. This  region has a visible cluster of crash incidents, and it is likely due to heavy traffic, dense  urbanization, and the fact that it is a hub for several major roads and expressways. These actions  could be directed towards improving traffic safety in this area to help prevent accidents.

**Step 3:** Next, we will begin "Severity Mapping." We'll plot crashes on the map and code them based on severity, distinguishing between crashes that resulted in injuries and those that led to fatalities. This will give us a visual representation of where severe crashes tend to occur. </br>

You may initially want to code these incidents by using different colors, but it's important to think about your map and how accessible it is. Will a color-coded map be easy to read for everyone? Let's make a map that's inclusive for people with color blindness by also creating differently-shaped markers (squares, circles, and triangles) for crashes, injuries, and fatalities.
"""

#TODO: Continue building your heatmap
import folium



# Filter to increase visibility of fatalities
fatalities = data_geo[data_geo['NUMBER OF PERSONS KILLED'] > 0]
injuries = data_geo[(data_geo['NUMBER OF PERSONS INJURED'] > 0) & (data_geo['NUMBER OF PERSONS KILLED'] == 0)].sample(n=500, random_state=42)  # Adjust sample size as needed

# Combine the samples to maintain a balanced view
sample_data_severity = pd.concat([fatalities, injuries])


# Create a base map
m_severity = folium.Map(location=[40.730610, -73.935242], zoom_start=10)



# Add crashes to the map with color coding and shape coding based on severity
for index, row in sample_data_severity.iterrows():
   if row['NUMBER OF PERSONS KILLED'] > 0:
       # Fatalities: Red can be a good color, using triangles for fatalities
       folium.features.RegularPolygonMarker(
         location=[row['LATITUDE'], row['LONGITUDE']],
         number_of_sides=3,  # Triangle
         radius=5,
         gradient=False,
         color='red',
         fill=True,
         fill_color='red'
       ).add_to(m_severity)


   elif row['NUMBER OF PERSONS INJURED'] > 0:
       # Injuries: Blue can be a good color, using circles for injuries
       folium.CircleMarker(
         location=[row['LATITUDE'], row['LONGITUDE']],
         radius=5,
         color='blue',
         fill=True,
         fill_color='blue'
      ).add_to(m_severity)
   else:
       # No injuries or fatalities: Green can be a good color, using squares for no injuries/fatalities
       folium.features.RegularPolygonMarker(
         location=[row['LATITUDE'], row['LONGITUDE']],
         number_of_sides=4,  # Square
         radius=5,
         gradient=False,
         color='green',
         fill=True,
         fill_color='green'
       ).add_to(m_severity)


# Save the map
m_severity.save("severity.html")

"""**TO DO:** On the left side of your screen, you will see an icon that represents a folder or a file. Follow the same steps as above to download the "Severity.html" file.

**TO DO:** Which intersection(s) seem to be the most dangerous?

> * The following is an analysis of traffic accidents in New York City, which reveals that the highest concentration of crashes occurs in Manhattan, particularly in the lower and midtown areas. Both pedestrians and vehicular traffic make this area notorious for severe accidents. There is a significant concentration of crashes in the northern parts of Brooklyn, which are near Queens. These are areas with complex interchanges and heavy traffic, leading to a high number of incidents. Severe crashes also occur along the connectivity routes, which include the major roads and highways linking different boroughs, such as the Brooklyn-Queens Expressway. These routes have high speeds and traffic volumes, which increase the risk of severe crashes.

---
---
"""

import pandas as pd

# Assuming your cleaned DataFrame is named 'cleaned_data'
# Display the first few rows of the cleaned data to review its structure and contents
print(cleaned_data.head())

"""##<font color='crimson'>  **Milestone #5 - Self-Guided Research Question**</font>
GOAL: In this Milestone, you will be prompted to take what youâ€™ve learned throughout this project, build your own research question, and create a visualization(s) or model(s) to support your research goals.

You may create your visualization(s) in this Colab Notebook, or in Excel, Tableau, PowerBI, etc. Choose whichever medium you are most comfortable with! Be creative!

For participants who are comfortable with advanced data science techniques, we welcome you to leverage additional datasets, if desired. We highly recommend using pre-cleaned datasets from open data sources, like Kaggle.com.

If you have any questions or get stuck, please email nsdc@nebigdatahub.org with your queries. We're here to help!

**Step 1:** Review the dataset(s) that you will be using. As you explore, [consider the research question you want to answer](https://libraries.indiana.edu/sites/default/files/Develop_a_Research_Question.pdf)! Additionally, think about [who you are telling your data's story to](https://hbr.org/2013/04/how-to-tell-a-story-with-data). Your final audience may contain a group of transportation professionals, data scientists, peers, and the general public. Think about how would you frame your analysis differently for each of these groups.

**TO DO:** List one or more research questions here that you are considering.

> *

**Step 2:** Now, think about what type of analysis you'd like to complete. Are you interested in looking at time series forecasting? Do you have additional maps in mind that you'd like to create? Is there a certain zip code or region you'd like to dive deeper into?

If you happen to be stuck, here are some examples that you can use or can guide you in choosing your research question!

**Examples:**
- How many crashes, injuries, and/or fatalies occurred in a zip code of interest?
- Which zip code sees the highest amount of crashes and what recommendations can you offer to help that community? Is it an underserved community?
- Do more crashes occur in underrepresented communities? Support your conclusion.
- Which day of the week sees the most crashes, injuries, and/or fatalities? (Hint: use the same method we used when we were analyzing the average number of crashes at different times of the day!)
- Does the geometric features of an intersection (90 degree intersection vs skewed intersection) affect the number of crashes that occur?

Be creative and think outside the box!

**Step 3:** Now that you've decided on your transportation research question, [explore the various types of visualizations you can create to support your research](https://datavizcatalogue.com/). You may create visualizations in this Google Colab notebook, Excel, R, SQL, PowerBI, Tableau, etc. Choose a program you are comfortable with!

You may also choose to build a model or leverage a different data science technique based on your experience level.

**Step 4:** Consider the **accessibility** of the graphs, charts, maps, or models you are interested in building. Use the tools below to learn more!
* How does your visualization appear to people [who may not be able to distinguish between muted colors or see your chart at all?](https://chartability.fizz.studio/)
*[Color Contrast Checker](https://policyviz.com/2022/11/01/color-contrast-checker-in-excel/)
*[SAS Graphics Accelerator](https://support.sas.com/software/products/graphics-accelerator/index.html)
*[TwoTone Data Sonification Tool](https://twotone.io/about/)
*[Making Visual Studio Accessible](https://code.visualstudio.com/docs/editor/accessibility)

To make visualizations more inclusive for people with color blindness, you can choose a color palette that is colorblind-friendly. `Seaborn`, a Python visualization library, provides several palettes that are designed to be perceptible by those with color vision deficiencies. Seaborn's `cubehelix` palette is a good choice, as it was designed specifically with color blindness in mind.

**Step 5:** Begin your research! Give yourself plenty of time to build your visualization or model. If you have any questions along the way, please email nsdc@nebigdatahub.org or write a message in the #[tdsp-community Slack Channel](https://join.slack.com/t/nsdcorps/shared_invite/zt-1h64t1e2p-La0AgU_HhymWUEGFQEcb3w).

**TO DO:** List the research question(s) you've chosen and why! Maybe you chose this question because it can help a community of interest or because it is similar to research you've completed in a class setting. Share your thoughts below.

> *

**TO DO:** Build a visualization, model, or use other statistical methods to gain insights into your data and to support your research question.
"""

#TO DO: Begin creating here!

"""---

##<font color='crimson'>**Milestone #6 - Virtual Poster Board Creation: Data Storytelling**</font>

GOAL: The main goal of this milestone is to create a one page, virtual poster board to portray your research findings and recommendations! Your poster may be shared with the Department of Transportation and Federal Highway Authority.

Within your poster, summarize your research question, your reasoning for selecting your data visualization or model choices, and key insights from your data analysis. You may also wish to include your outstanding research questions that could not be answered by the dataset and why.

**Be sure to answer the following on your virtual poster board:** Based on your research insights, what recommendations would you share with the Department of Transportation and Federal Highway Authority to make roads safer for vulnerable road users? Why?

**Additionally, be sure to cite all relevant sources that you referred to throughout this project on your poster board (MLA or APA citation preferred). List acknowlegdments if you received any support from mentors, professors, professionals, etc. throughout your journey.**

Please use the following resources to get started!


*   [Virtual Poster Board Template](https://nebigdatahub.org/wp-content/uploads/2024/01/Copy-of-dsi-poster.ppt-48-Ã—-36-in.pdf) - Your one-page, virtual poster may be created in PowerPoint, Google Slides, Canva, etc. Choose what you are most comfortable with!
* [ Data Storytelling: How to Effectively Tell a Story with Data](https://online.hbs.edu/blog/post/data-storytelling)

* [  Consider how your visualization(s) might appear to people with varying abilities ](https://chartability.fizz.studio/)
*  [Understand your audience for an optimal presentation](https://hbr.org/2013/04/how-to-tell-a-story-with-data)


Once completed, please use the [following TDSP Submission Form](https://docs.google.com/forms/d/e/1FAIpQLSeX1OSHj58EQs4ypFEPB_SH3OpWZeo67yU0WWOPVSqYtDrpWg/viewform) to share your Google Colab Notebook and your one-page, virtual project poster with the NSDC HQ Team.

---
---

## ðŸš—<font color='crimson'> **Thank you for completing the project!**</font> ðŸš—

We are one step closer to making roads safer for all. [Please submit all materials to the NSDC HQ team](https://docs.google.com/forms/d/e/1FAIpQLSeX1OSHj58EQs4ypFEPB_SH3OpWZeo67yU0WWOPVSqYtDrpWg/viewform) in order to receive a certificate of completion. Do reach out to us if you have any questions or concerns. We are here to help you learn and grow.
"""